# -*- coding: utf-8 -*-
"""CreditCardFraudDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ijt9YYf0rKyUr7xtcSHrqdR48RXIpv_I
"""

# Install required packages with specific versions
!pip install xgboost==1.7.3 imbalanced-learn scikit-learn==1.2.2 pandas numpy matplotlib seaborn

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                           roc_curve, auc, precision_recall_curve)
from imblearn.over_sampling import SMOTE
import logging
import pickle
import warnings
warnings.filterwarnings('ignore')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class FraudDetectionModel:
    def __init__(self): # Change _init_ to __init__
        """
        Initialize the FraudDetectionModel with necessary attributes
        """
        self.data = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.best_model = None
        self.models = {
            'Logistic Regression': LogisticRegression(max_iter=1000),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'XGBoost': XGBClassifier(
                n_estimators=100,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )
        }

    def load_data(self, file_path):
        """
        Load and perform initial data validation
        """
        try:
            self.data = pd.read_csv(file_path)
            # Basic validation
            if self.data.empty:
                raise ValueError("The loaded dataset is empty")
            if 'Class' not in self.data.columns:
                raise ValueError("'Class' column not found in the dataset")

            print(f"Data loaded successfully. Shape: {self.data.shape}")
            return True
        except FileNotFoundError:
            print(f"Error: File not found at {file_path}")
            return False
        except Exception as e:
            print(f"Error loading data: {str(e)}")
            return False

    def explore_data(self):
        """
        Perform exploratory data analysis with visualizations
        """
        try:
            if self.data is None:
                raise ValueError("No data loaded. Please load data first.")

            # Print basic information
            print("\nDataset Shape:", self.data.shape)
            print("\nClass Distribution:")
            print(self.data['Class'].value_counts())
            print("\nBasic Statistics:")
            print(self.data.describe())

            # Visualizations
            plt.figure(figsize=(15, 6))

            # Class distribution plot
            plt.subplot(1, 2, 1)
            sns.countplot(data=self.data, x='Class')
            plt.title('Class Distribution (0: Normal, 1: Fraud)')
            plt.xlabel('Class')
            plt.ylabel('Count')

            # Correlation heatmap
            plt.subplot(1, 2, 2)
            sns.heatmap(self.data.corr(), cmap='coolwarm', annot=False)
            plt.title('Correlation Heatmap')

            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Error in data exploration: {str(e)}")

    def preprocess_data(self):
        """
        Preprocess the data including handling imbalance
        """
        try:
            if self.data is None:
                raise ValueError("No data loaded. Please load data first.")

            # Handle missing values
            self.data = self.data.dropna()

            # Separate features and target
            self.X = self.data.drop('Class', axis=1)
            self.y = self.data['Class']

            print("Starting data preprocessing...")
            print(f"Original dataset shape: {self.X.shape}")

            # Handle imbalanced data using SMOTE
            print("\nApplying SMOTE to handle imbalanced data...")
            print("Before SMOTE - Class distribution:", dict(zip(*np.unique(self.y, return_counts=True))))

            smote = SMOTE(random_state=42)
            X_balanced, y_balanced = smote.fit_resample(self.X, self.y)

            print("After SMOTE - Class distribution:", dict(zip(*np.unique(y_balanced, return_counts=True))))

            # Split data
            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
                X_balanced, y_balanced, test_size=0.2, random_state=42
            )

            # Scale features
            self.X_train = self.scaler.fit_transform(self.X_train)
            self.X_test = self.scaler.transform(self.X_test)

            print(f"Training set shape: {self.X_train.shape}")
            print(f"Testing set shape: {self.X_test.shape}")

            return self.X_train, self.X_test, self.y_train, self.y_test

        except Exception as e:
            print(f"Error in data preprocessing: {str(e)}")
            return None, None, None, None

    def train_models(self, X_train, y_train):
        """
        Train and compare three models with visualization
        """
        try:
            if X_train is None or y_train is None:
                raise ValueError("Training data not provided")

            results = {}
            print("\nTraining and evaluating models...")

            for name, model in self.models.items():
                print(f"\nTraining {name}...")
                try:
                    # Perform cross-validation
                    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
                    results[name] = {
                        'cv_mean': cv_scores.mean(),
                        'cv_std': cv_scores.std()
                    }
                    print(f"{name} - F1 Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
                except Exception as model_error:
                    print(f"Error training {name}: {str(model_error)}")
                    continue

            if not results:
                raise ValueError("No models were successfully trained")

            # Select best model
            best_model_name = max(results, key=lambda k: results[k]['cv_mean'])
            self.best_model = self.models[best_model_name]
            print(f"\nBest performing model: {best_model_name}")

            # Train best model on full training set
            self.best_model.fit(X_train, y_train)

            # Visualize model comparison
            self._plot_model_comparison(results)

            return results

        except Exception as e:
            print(f"Error in model training: {str(e)}")
            return None

    def _plot_model_comparison(self, results):
        """
        Helper method to plot model comparison
        """
        try:
            plt.figure(figsize=(10, 6))
            models_cv_mean = [results[model]['cv_mean'] for model in results.keys()]
            models_cv_std = [results[model]['cv_std'] for model in results.keys()]

            bars = plt.bar(range(len(results)), models_cv_mean, yerr=models_cv_std,
                          capsize=5, alpha=0.7)
            plt.xticks(range(len(results)), list(results.keys()), rotation=45)
            plt.title('Model Comparison - Cross Validation F1 Scores')
            plt.xlabel('Models')
            plt.ylabel('F1 Score')

            # Add value labels on bars
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.3f}', ha='center', va='bottom')

            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"Error plotting model comparison: {str(e)}")

    def tune_hyperparameters(self, X_train, y_train):
        """
        Perform hyperparameter tuning for the best model
        """
        try:
            if self.best_model is None:
                raise ValueError("No best model selected. Please train models first.")

            print("\nTuning hyperparameters for the best model...")

            param_grids = {
                'LogisticRegression': {
                    'C': [0.001, 0.01, 0.1, 1, 10],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'saga']
                },
                'RandomForestClassifier': {
                    'n_estimators': [100, 200],
                    'max_depth': [10, 20, None],
                    'min_samples_split': [2, 5]
                },
                'XGBClassifier': {
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.01, 0.1],
                    'n_estimators': [100, 200],
                    'min_child_weight': [1, 3, 5],
                    'subsample': [0.8, 0.9, 1.0]
                }
            }

            # Select appropriate param grid based on best model type
            model_type = type(self.best_model).__name__ # Change _name_ to __name__
            param_grid = param_grids.get(model_type)

            if param_grid is None:
                raise ValueError(f"No parameter grid defined for model type: {model_type}")

            grid_search = GridSearchCV(
                estimator=self.best_model,
                param_grid=param_grid,
                cv=5,
                scoring='f1',
                n_jobs=-1
            )

            print("Performing grid search...")
            grid_search.fit(X_train, y_train)

            print(f"\nBest parameters found: {grid_search.best_params_}")
            print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

            self.best_model = grid_search.best_estimator_
            return grid_search.best_params_

        except Exception as e:
            print(f"Error in hyperparameter tuning: {str(e)}")
            return None

    def evaluate_model(self, X_test, y_test):
        """
        Evaluate the model using multiple metrics
        """
        try:
            if self.best_model is None:
                raise ValueError("No model to evaluate. Please train a model first.")

            print("\nEvaluating final model...")
            y_pred = self.best_model.predict(X_test)
            y_pred_proba = self.best_model.predict_proba(X_test)[:, 1]

            # Calculate metrics
            print("\nClassification Report:")
            print(classification_report(y_test, y_pred))

            # Plot evaluation curves
            self._plot_evaluation_curves(y_test, y_pred_proba)

            return {
                'accuracy': accuracy_score(y_test, y_pred),
                'classification_report': classification_report(y_test, y_pred),
                'confusion_matrix': confusion_matrix(y_test, y_pred)
            }

        except Exception as e:
            print(f"Error in model evaluation: {str(e)}")
            return None

    def _plot_evaluation_curves(self, y_test, y_pred_proba):
        """
        Helper method to plot ROC and Precision-Recall curves
        """
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

            # ROC Curve
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
            ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            ax1.set_xlabel('False Positive Rate')
            ax1.set_ylabel('True Positive Rate')
            ax1.set_title('ROC Curve')
            ax1.legend()

            # Precision-Recall curve
            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
            ax2.plot(recall, precision, color='blue', lw=2)
            ax2.set_xlabel('Recall')
            ax2.set_ylabel('Precision')
            ax2.set_title('Precision-Recall Curve')

            plt.tight_layout()
            plt.show()
        except Exception as e:
            print(f"Error plotting evaluation curves: {str(e)}")

    def save_model(self, filepath):
        """
        Save the trained model to Google Drive
        """
        try:
            if self.best_model is None:
                raise ValueError("No model to save. Please train a model first.")

            model_data = {
                'model': self.best_model,
                'scaler': self.scaler
            }

            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"Model saved successfully to {filepath}")
            return True

        except Exception as e:
            print(f"Error saving model: {str(e)}")
            return False

    @staticmethod
    def load_model(filepath):
        """
        Load a trained model from Google Drive
        """
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            print(f"Model loaded successfully from {filepath}")
            return model_data
        except Exception as e:
            print(f"Error loading model: {str(e)}")
            return None

def run_fraud_detection():
    """
    Main function to run the complete fraud detection pipeline
    """
    try:
        # Initialize model
        print("Initializing Fraud Detection Model...")
        fraud_detector = FraudDetectionModel()

        # Update this path to your Google Drive path where the dataset is stored
        data_path = '/content/drive/MyDrive/Colab Notebooks/creditcard2.csv'

        print(f"\nAttempting to load data from: {data_path}")
        if not fraud_detector.load_data(data_path):
            raise ValueError("Failed to load data")

        print("\nStarting fraud detection pipeline...")

        # Step 1: Exploratory Data Analysis
        print("\nStep 1: Performing Exploratory Data Analysis...")
        fraud_detector.explore_data()

        # Step 2: Preprocess Data
        print("\nStep 2: Preprocessing data...")
        X_train, X_test, y_train, y_test = fraud_detector.preprocess_data()

        # Step 3: Train Models
        print("\nStep 3: Training models...")
        training_results = fraud_detector.train_models(X_train, y_train)

        # Step 4: Tune Hyperparameters
        print("\nStep 4: Tuning hyperparameters...")
        best_params = fraud_detector.tune_hyperparameters(X_train, y_train)

        # Step 5: Final Evaluation
        print("\nStep 5: Performing final evaluation...")
        evaluation_results = fraud_detector.evaluate_model(X_test, y_test)

        # Step 6: Save Model
        save_path = '/content/drive/MyDrive/fraud_detection_model.pkl'
        print(f"\nStep 6: Saving model to {save_path}...")
        if fraud_detector.save_model(save_path):
            print("Pipeline completed successfully!")

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        logging.error(f"Error in pipeline execution: {str(e)}")

# Run the fraud detection pipeline
if __name__ == "__main__": # Change '_name_' to '__name__'
    run_fraud_detection()